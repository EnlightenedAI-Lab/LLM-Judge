# **Reflective Alignment Datasets**

The Enlightened AI Research Lab maintains a growing collection of research-grade  
datasets designed to evaluate **internal coherence stability**, **reflective reasoning**,  
and **perturbation resilience** in advanced language models.

All datasets support the Reflective Alignment Architecture (RAA) and the  
Reflective Duality Layer (RDL).

---

## **1. Reflective Alignment Dataset (v1)**  
**Zenodo DOI:** https://zenodo.org/records/17751263  

A curated collection of prompts engineered for evaluating:

- reflective reasoning drift  
- internal contradiction patterns  
- perturbation-response stability  
- multi-layer coherence dynamics  

### Contains:
- ~300 foundational reflective prompts  
- paired forward vs reflective test sets  
- contradiction pressure tests  
- divergence chains  
- temporal offset tests  

This dataset forms the backbone of **LLM Judge (L1–L2)** evaluations.

---

## **2. Expansion Sets (Upcoming)**

Throughout 2025–2026, we are extending the dataset suite to include:

### **Ethical Ambiguity Chains**
Long-horizon moral reasoning sequences designed to test  
value consistency and reflective justification behavior.

### **Stress-Test Perturbation Sets**
Paired inputs with noise, contradiction, or temporal distortion  
to evaluate breakdown points and collapse gradients.

### **Self-Critique & Rationalization Chains**
Multi-step reflective reasoning sequences that expose  
rationalization artifacts, collapse loops, and stability thresholds.

### **Consistency Loops**
Brittle-logic tests constructed to measure the  
*Moral Coherence Index (MCI★)* and cross-layer rigidity.

---

## **3. Alignment Benchmarks (Future Releases)**

We are working toward the first full **Reflective-Alignment Benchmark Suite**, including:

- RDL stability indices (Ψ profiles)
- Coherence-collapse mapping
- Signature clustering datasets
- Failure-mode atlases for reflective reasoning

These benchmarks will be openly available for labs, universities,  
and safety groups seeking transparent model evaluation instruments.

---

## **4. Contributing Data**

We invite contributions from:

- research labs  
- academic groups  
- industry AI teams  
- safety organizations  

### Contribution Guidelines:
All contributions must follow:

- reproducibility standards  
- annotation schemas  
- reflective-evaluation formatting  

See **CONTRIBUTING.md** for full instructions.

---

## **Contact**

**Email:** research@enlightenedai.ai  
**Website:** https://www.enlightenedai.ai  
**GitHub:** https://github.com/EnlightenedAI-Lab
